---
title: 第二章 - 信息的度量
date: 2022-02-28T15:00:00+08:00
categories: 信息论
layout: note
article: false
---
## 2-1 度量信息的基本思路

**定义 2.1**：若信源发出的消息是离散的、有限或无限可数的符号或数字，且一个符号代表一条完整的消息，则为**单符号离散信源**

**定义 2.2**： 若信源的输出是随机事件 X，其出现概率为 $P(X)$，则它们所构成的集合称为 **信源的概率空间** 或 **信源空间**

信源空间的描述：
$$[X\cdot P]:\begin{cases}
   X:&x_1,&x_2,&\dots,&x_N\\
   P(X):&P(x_1),&P(x_2),&\dots,&P(x_N)
\end{cases}$$

有 $\sum_{i=1}^N P(x_i)=1$

信源输出的事件的概率越小，信息量越大，即信息量是概率的减函数

自信息量 $I(x_i)$ 表示 $x_i$ 发生所带来的信息量

**定义 2.3** ==自信息量 $I(x_i)=\log\frac{1}{P(x_i)}=-\log P(x_i)$==

信息量的单位：
- 以 2 为底：比特 bit
- 以 e 为底；奈特 nat
- 以 10 为底：哈特 Hart
- $1 nat \approx 1.44 bit, 1 Hart \approx 3.32 bit$

### 2-2-1 信源熵

信源熵：$H(X)=\sum_{x\in X} P(x)I(x)=\sum^N_{i=1}P(x_i)I(x_i)=-\sum_{i=1}^N\log P(x_i)$

- 信源熵只与信源符号概率分布有关，是一种先验熵
- 对给定概率分布的信源，信源熵是定值，代表信源每发出一个符号给出的平均信息量，其量纲为 信息量单位 / 信源符号

## 2-3 互信息量和平均互信息量


